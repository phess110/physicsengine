<!DOCTYPE html>
<html>
<head>
    <title>Entropy & Information</title>
    <link rel="stylesheet" href="style.css">

    <!--- JQuery --->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script>
        $.get("nav.html", function(data){
            $("#nav-container").replaceWith(data);
        });
    </script>

    <!--- LaTeX Math Renderer --->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
</head>

<body>
    <div id="nav-container"></div>

    <h2>Shannon Information Entropy</h2>
    <p class="text">
      Suppose we're performing an experiment with a set of possible outcomes \(P_1,\ldots,P_n\) having probabilities \(p_1,\ldots,p_n\). We want to find a measure of how uncertain we are of the outcome of such an experiment, or equivalently, how much information we'd receive, in expectation, should we observe the outcome. Let \(H=H_n(p_1,\ldots,p_n)\) be such a measure. For example, suppose we take \(n=2\) and \(p_1=1\) and \(p_2 = 0\). Then, there is 0 uncertainty in the outcome of the experiment and we receive no information by performing it, so \(H(1,0) = 0\). On the other hand, if \(p_1=p_2=\frac{1}{2}\), then intuitively we are as uncertain as possible given that we know there are only 2 possible outcomes, so \(H(\frac{1}{2},\frac{1}{2}) \geq H(p,1-p)\) for all \(p\in [0,1]\).

      Define \(A(n) = H_n(\frac{1}{n},\ldots,\frac{1}{n})\), that is, \(A(n)\) is the level of uncertainty when choosing amongst \(n\) equally likely outcomes. It's natural to require the following two properties of \(H\):
    </p>

    <ul>
      <li>The uncertainty of the outcome is independent of how the experiment is conducted.
          In particular, if a choice is broken down into successive choices the uncertainty is the same as if it hadn't been broken down.
          Formally, say that \(p_i=\sum_{j=1}^k   P(Q_j)P(P_i|Q_j)=\sum_{j=1}^k q_jr_{i,j}\), i.e. we make a choice amongst \(k\) outcomes, \(Q_j\), with probabilities \(q_j\), and the probability of observing \(P_i\) given \(Q_j\) is \(r_{i,j}\). Then \[H(p_1,\ldots,p_n) = H(q_1,\ldots,q_k) + \sum q_j H(r_{1,j},\ldots,r_{n,j}).\]
          More succinctly, we can just say the \(H\) respects the law of total probability.
      </li>
      <li>\(A(n)\) is monotonically increasing in \(n\); as the number of possible equally likely choices increases, the more uncertain we are.</li>
    </ul>

    <p class="text">
        From condition 1, we can decompose a choice of \(s^m\) equally likely possibilities into a series of \(m\) choices of \(s\) equally likely possibilities. Thus \[A(s^m)= mA(s).\]

        Likewise \(A(t^n) = nA(t)\). Choose \(n\) arbitrarily large and find an \(m\) such that \[s^m \leq t^n < s^{m+1}.\]Taking logarithms and dividing by \(n\log s\), we have \[\frac{m}{n} \leq \frac{\log t}{\log s} \leq \frac{m}{n}+\frac{1}{n}, \text{ i.e. } \left| \frac{m}{n} - \frac{\log t}{\log s}\right| < \varepsilon.\]

        By the monotonicity of \(A(n)\), we have \(A(s^m)\leq A(t^n) \leq A(s^{m+1})\), or \[mA(s) \leq nA(t) \leq (m+1)A(s).\]Dividing by \(nA(s)\), we obtain \[\left| \frac{m}{n} - \frac{A(t)}{A(s)} \right| < \varepsilon.\]Whence, \[\left|\frac{A(t)}{A(s)} - \frac{\log t}{\log s}\right | < 2\varepsilon\]or \(A(t) = K\log t\), where \(K\) is an arbitrary positive constant.

        Now suppose we have \(n\) possibilities with probabilities, \(p_i = \frac{n_i}{N}\) where \(N = \sum n_i\) and \(n_i\in \mathbb{Z}_{\geq 0}\). Then we can break down a choice from \(N\) equally likely possibilities into a choice from \(n\) possibilities with probability \(p_i\), respectively. Hence by condition 1 we have, \[A(N) = K\log N = H_n(p_1,\ldots,p_n) + K\sum p_i\log n_i,\]from which we obtain 
        \[\begin{aligned}
        H &= K\left(\sum p_i\log N - \sum p_i\log n_i\right)\\
        &= -K\sum p_i\log \frac{n_i}{N}\\
        &= -K\sum p_i\log p_i
        \end{aligned}\]

        If we further assume \(H\) is continuous, then this also holds when \(p_i\) are not necessarily rational.
    </p>
</body>
</html>