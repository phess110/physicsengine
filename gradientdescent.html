<!DOCTYPE html>
<html>
<head>
    <title>Optimization</title>
    <link rel="stylesheet" href="style.css">

    <!--- JQuery insert menu --->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script>
        $.get("nav.html", function(data){
            $("#nav-container").replaceWith(data);
        });
    </script>

    <!--- LaTeX Math Renderer --->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
</head>

<body>
    <div id="nav-container"></div>
    <h1>Optimization Methods</h1>
    <h2>Taylor Approximations</h2>

    <ul>
        <li> If \(f: \mathbb{R} \to \mathbb{R} \), then \(f(x+\Delta x) = f(x) + \Delta x f'(x) + \frac{1}{2}\Delta x^2 f''(x).\) </li>
        <li> If \(F: \mathbb{R}^n \to \mathbb{R}\), then \(F(\mathbf{x}+\mathbf{\Delta x}) = F(\mathbb{x}) + \mathbf{\Delta x}^T \nabla F(\mathbf{x}) + \frac{1}{2} \mathbf{\Delta x}^T H \mathbf{\Delta x}\), where \(H\)
            is the \(n\times n\) Hessian matrix, where $$H_{ij} = \frac{\partial F }{\partial x_i\partial x_j} = H_{ji}.$$ </li>
        <li> If \(\mathcal{F}(\mathbf{x}) = (f_1(\mathbf{x}), \ldots, f_m(\mathbf{x}))\), where \(f_i: \mathbb{R}^n \to \mathbb{R}\), then 
            \(\mathcal{F}(\mathbf{x+\Delta x}) = \mathcal{F}(\mathbf{x}) + J \mathbf{\Delta x}\) where \(J\) is the \(m\times n\) Jacobian matrix where 
            $$J_{ij} = \frac{\partial f_i}{x_j}.$$
        </li>
    </ul>


    <h2>Newton's Method</h2>
    <p class="text" id="Newtons">
    The Taylor approximation gives
        $$\mathcal{F}(\mathbf{x}_{k+1}) = \mathcal{F}(\mathbf{x}_{k}) + J(\mathbf{x}_k) (\mathbf{x}_{k+1} - \mathbf{x}_k).$$
    If we use this to take a step closer towards a zero, we can approximate the LHS as 0 and we obtain the Newton iteration: 

    $$ \mathbf{x}_{k+1} = \mathbf{x}_{k}  - J^{-1}(\mathbf{x}_{k})\mathcal{F}(\mathbf{x}_{k})$$
    </p>

    <h2>Minimization</h2>

    <p class="text" id="optimize">
        To minimize \(F(\mathbf{x})\), we need to solve \(\nabla F(\mathbf{x}) = 0\). 
    </p>
    <ul>
        <li> <b>Steepest Descent.</b> \(s_k\) is the step size or learning rate. We can use line search (move in direction of
             the gradient until we find the minimum along the line) to find the optimal \(s_k\). 
             $$ \mathbf{x}_{k+1} = \mathbf{x}_{k} - s_k \nabla F(\mathbf{x}_k).$$</li>

        <li> <b>Newton's.</b> The Hessian is the Jacobian of the gradient. $$\mathbf{x}_{k+1} = \mathbf{x}_{k} - H^{-1}(\mathbf{x}_{k})\nabla F(\mathbf{x}_{k}).$$ Newton's method is 
            fast since it provides quadratic convergence; however, computing the Hessian is often too expensive.</li>
    </ul>

    <p class="text" id="convex">
        We generally focus on optimizing convex functions since convexity prevents multiple local minima. A function is 
        convex if the points on or above the graph form a convex set OR the Hessian is positive semidefinite.
    </p>

    <h3>Steepest Descent Convergence</h3>
    <p class="text" id="steep-conv">
        Recall \(\mathbf{x}_{k+1} = \mathbf{x}_{k} - s \nabla F(\mathbf{x}_k)\). Let \(H\) be the Hessian
        of \(F\). Assuming \(F\) is strictly convex, the eigenvalues of \(H\) satisfy \(0\lt m \lt \lambda \lt M\) for all \(x\); one can show

        $$ F(\mathbf{x}_{k+1}) - F(\mathbf{x}^*) \leq \left(1 - \frac{m}{M}\right)(F(\mathbf{x}_{k}) - F(\mathbf{x}^*)).$$

        This is linear convergence.
    </p>

    <h4>Inexact Line Search</h4>
    <p class="text" id="line-search">
        Take \( \alpha \lt \frac{1}{2}\) and \(\beta \lt 1\). Set \(s=1\) and \(X = \mathbf{x}_k - s\nabla F\).
        If \(F(X) \leq F(\mathbf{x}_k) - \alpha s |\nabla F|^2\), stop and take \(\mathbf{x}_{k+1} = X\). Else
        backtrack with \(s \gets \beta s\).

        While exact line search gives the reduction factor \(1 - m/M\), inexact line search gives 
        $$1 - \min(2m\alpha, 2m\alpha\beta/M).$$
    </p>


    <h2>Adding Momentum</h2>
    <p class="text" id="momentum">
        Prevents gradient descent from 'zig-zagging' too much on its way to the minimum.
         $$\mathbf{x}_{k+1} = \mathbf{x}_k -s\mathbf{z}_k \mathrm{\quad  where \quad} \mathbf{z}_k = \nabla F(\mathbf{x}_k) + \beta \mathbf{z}_{k-1}.$$


    </p>
</body>
</html>