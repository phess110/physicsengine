<!DOCTYPE html>
<html>
<head>
    <title>Chi-Squared</title>
    <link rel="stylesheet" href="style.css">

    <!--- JQuery --->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script>
        $.get("nav.html", function(data){
            $("#nav-container").replaceWith(data);
        });
    </script>

    <!--- LaTeX Math Renderer --->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
</head>

<body>
    <div id="nav-container"></div>

    <h2>\(\chi^2\)-Test</h2>
    <p class="text">
        Imagine you were handed a random die and you had to figure out whether the die was fair or not. It's quite likely the first thing you would do 
        is start throwing the die and tallying the outcomes. After sufficiently many throws, if the distribution of the tallies was approximately uniform, then you'd be relatively confident the die is indeed unbiased.
        <br>       
        <br> 
        However, we'd like a more formal measure that accounts for the fact that even fair die will deviate from producing a perfectly uniform sampling (and sometimes by a significant amount).
        This is where the \(\chi^2\)-test comes in. The fundamental parameters of a \(\chi^2\)-test are as follows:
    </p>

    <ul>
        <li>\(n\): the number of independent observations (in our case, how many times we threw the die)</li>
        <li>\(k\): the number of categories in our distribution (in our case, this would be the number of distinct sides on the die)</li>
        <li>\(p_i\): the probability each observation falls into category \(i\) for the hypothesized distribution (in our case, the hypothesized distribution would be the uniform distribution)</li>
        <li>\(Y_i\): the actual number of observations of category \(i\) in our sample.</li>
    </ul>

    <p class="text">
        With this in mind we define the following statistic: $$V = \sum_{i=1}^n \frac{(Y_i-np_i)^2}{np_i}.$$
        Note if the die indeed followed the hypothesized distribution, then \(np_i\) is the expected number of observations of category \(i\). Thus
        $$\sum_{i=1}^n (Y_i-np_i)^2$$ can be viewed as the "variance" of our sample. So what's the deal with the \(np_i\) in the denominator. Well, for certain distributions
        some outcomes may be much less likely than others. It would make sense to weight deviations with less likely outcomes more, so that our statistic provides a better measure of deviation from the norm.
        However, $$\sum_{i=1}^n (Y_i-np_i)^2$$ weights all categories equally. With our \(V\)-statistic however, more likely observations, i.e. ones where \(np_i\) is relatively big, will be weighted less than unlikely observations.
    </p>

    <p class="text" >
       Note that by expanding \((Y_i-np_i)^2\) and using the fact that \(\sum Y_i = n\) and \(\sum p_j = 1\), we can rewrite \(V\) as follows: 
       $$ V = \left(\frac{1}{n} \sum_{i=1}^n \left(\frac{V_i^2}{p_i}\right)\right) - n$$
    </p>

    <p class="text" >
        Now, note that \(V \geq 0\), and \(V\) being close to 0 indicates that our hypothesized distribution is correct. However, the question remains, just small 
        should we want \(V\) to be. To answer this question, we'll need to derive the chi-squared distribution. Some sample values of the distribution are shown below.
    </p>

    <div class="padbottom">
        <embed src="images/table.pdf" height="550px" class="center"/>
    </div>

    <p class="text">
        First, we'll need to understand what is meant by the degrees of freedom in our distribution.
        We say the distribution of \(V\) has \(k-1\) degrees of freedom because \(\sum Y_i = n\), i.e. 
        if I tell you the values of \(k-1\) of the \(Y_i\),
        then the value of the remaining \(Y_s\) can be determined uniquely and thus \(V\) is also determined.
        It is common to denote the number of degrees of freedom by \(\nu = k-1.\)

        The table entry \(x\) under column \(p\) and row \(\nu = k-1\) means 
        
        $$\textrm{Pr}\hspace{-0.9em}(V \leq x) \approx p$$
        for sufficently large \(n\). For example, the 99% entry in row 8 is 20.09.
         Hence, with 8 degrees of freedom, \(V > 20.09\) only 1% of the time.
         
         So if we had a 9-sided die and we ran 1000 trials and we hypothesized the die was fair, but
         our calculated value of \(V\) given the trial data is greater than 20.09, we can 
         be fairly certain that the die actually wasn't actually fair and we reject our hypothesis.
    </p>

    <p class="text">
        So where do the numbers in the table above come from? 

        Assuming the distribution of the random variables \(Y_i\) was indeed our proposed hypothesis distribution given by the
        \(p_1,p_2,\ldots,p_k\), it's easily seen that 

        $$\textrm{Pr}\hspace{-0.9em}((Y_1,\ldots,Y_k) = (y_1,\ldots,y_k)) = \binom{n}{y_1,\ldots,y_k} p_1^{y_1}\cdots p_k^{y_k} = \frac{n!}{y_1! \cdots y_k!}p_1^{y_1}\cdots p_k^{y_k},$$
        (see the <a href="https://en.wikipedia.org/wiki/Multinomial_distribution">multinomial distribution</a>). Now, lets suppose
        \(Y_s=y_s\) with the <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson probability</a>
        $$ \frac{e^{np_s}(np_s)^{y_s}}{y_s!}$$ and that the \(Y\)'s  are independent random variables (note: this is not necessarily the case, but we'll see this assumption doesn't affect the final derived distribution).

        With this assumption,  
        $$\textrm{Pr}\hspace{-0.9em}((Y_1,\ldots,Y_k) = (y_1,\ldots,y_k)) = \prod_{s=1}^k \frac{e^{-np_s}(np_s)^{y_s}}{y_s!},$$
        and hence \(Y_1+\ldots+Y_k=n\) with probability 
        $$(\star) = \sum_{\substack{y_1+\cdots+y_k=n\\ \\ y_i \geq 0}} \prod_{s=1}^k \frac{e^{-np_s}(np_s)^{y_s}}{y_s!}.$$

        However, because \(\sum p_i =1\) and by the multinomial distribution  $$n!\sum_{\substack{y_1+\ldots+y_k=n\\ \\ y_i\geq 0}} \prod_{s=1}^k \frac{p_s^{y_s}}{y_s!} = 1,$$it follows that
        $$(\star) = \frac{e^{-n}n^n}{n!}.$$
        Therefore, if we assume that the \(Y\)'s  are independent except for the condition \(Y_1+\ldots+Y_k=n\), then by the definition of conditional probability,

        $$\textrm{Pr}\hspace{-0.9em}((Y_1,\ldots,Y_k) = (y_1,\ldots,y_k)) = \left.\left(\prod_{s=1}^k \frac{e^{-np_s}(np_s)^{y_s}}{y_s!}\right) \middle/ \left(\frac{e^{-n}n^n}{n!}\right)\right. = \binom{n}{y_1,\ldots,y_k} p_1^{y_1}\cdots p_k^{y_k}.$$
        This is the multinomial distribution again! Our assumption that the \(Y\)'s are Poisson doesn't matter.

    </p>

    <p class="text">
        Now, we make a change of variables. $$Z_s = \frac{Y_s -np_s}{\sqrt{np_s}}.$$ Note $$V = Z_1^2 + \ldots + Z_k^2$$ and the condition that 
        \(Y_1+\ldots+Y_k=n\) is equivalent to $$ \sqrt{p_1}Z_1 + \ldots + \sqrt{p_k}Z_k = 0.$$

        The solutions \((Z_1,\ldots,Z_k)\) to the equation \(\sqrt{p_1}Z_1 + \ldots + \sqrt{p_k}Z_k = 0\) form a \(k-1\)-dimension subspace, \(S\), of \(\mathbb{R}^{k}\) (in fact \(S\) is a hyperplane).
        For sufficiently large \(n\), each \(Z_s\) is approximated by a normal distribution (recall the central limit theorem). 

        Therefore, points within the differential volume \(\mathrm{d}z_1\cdots \mathrm{d}z_k\) of \(S\) occurs with probability approximately proportional to 
        $$\exp(-(z_1^2+\ldots+z_k^2)/2)$$(refer to the multivariate normal gaussian distribution). 
     </p>

     <p class="text">
        Now, the probability \(V\leq v\) is 
        $$\frac{\int_{(z_1,\ldots,z_k)\in S \text{and} z_1^2+\ldots+z_k^2 \leq v} \exp(-(z_1^2+\ldots+z_k^2)/2)\mathrm{d}z_1\cdots \mathrm{d}z_k}{\int_{(z_1,\ldots,z_k)\in S} \exp(-(z_1^2+\ldots+z_k^2)/2)\mathrm{d}z_1\cdots \mathrm{d}z_k}.$$Note that
        since \(S\) is a \(k-1\)-dimensional hyperplane through the origin, the region of integration in the numerator is the interior of a \(k-1\)-dimensional hypersphere centered at the origin.  
        
        Thus, a change to polar coordinates allows a nice simplification (we use \(\chi\) instead of \(r\) for the radius as Pearson did when he first derived this): 
        $$\frac{\int_{0}^{\sqrt{v}} \exp(-\chi^2/2)\chi^{k-2}\mathrm{d}\chi}{\int_{0}^{\infty} \exp(-\chi^2/2)\chi^{k-2}\mathrm{d}\chi}$$
        This reveals how the test got its name. The integrals above are more well-known as the incomplete gamma function and the gamma function, so we get 

        $$ \lim_{n\to \infty} \textrm{Pr}\hspace{-0.9em}(V \leq v) = \left.\gamma\left(\frac{k-1}{2},\frac{v}{2}\right) \middle/ \Gamma\left(\frac{k-1}{2}\right)\right.$$
    </p>

    <p class="text">
        Another question the remains is how large should we choose \(n\) to be when we apply this test. Empirically,
        it usually suffices to choose \(n\) large enough so that \(np_i \geq 5\) for each category \(i\).
    </p>
</body>
</html>